{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUGsKzNu1amS"
      },
      "source": [
        "# Sur Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1RBXR841MBD",
        "outputId": "7ae23702-2af8-4bbc-a783-ff06616fbe17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CFm-gAY1m2Y"
      },
      "source": [
        "# Importation des bibliothèques essentielles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vjIFbPRR1eAP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_5kYUpL13ZL"
      },
      "source": [
        "# Exploration de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1_FSmLg43OT",
        "outputId": "96f79bd1-7a05-46c5-a096-2a5ce3dfdd80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4933 entries, 0 to 4932\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Year      4933 non-null   int64 \n",
            " 1   Title     4933 non-null   object\n",
            " 2   Abstract  4933 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 115.7+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('nips.csv')\n",
        "df = df[df['Abstract'] != \"Abstract Missing\"]\n",
        "df = df.reset_index(drop=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmjh80Cg14W4",
        "outputId": "17201145-bfae-4b36-c9fc-a21674884940"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4933 entries, 0 to 4932\n",
            "Data columns (total 3 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Year      4933 non-null   int64 \n",
            " 1   Title     4933 non-null   object\n",
            " 2   Abstract  4933 non-null   object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 115.7+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/nips.csv')\n",
        "df = df[df['Abstract'] != \"Abstract Missing\"]\n",
        "df = df.reset_index(drop=True)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "69FO9uFt43OV",
        "outputId": "b957e196-73c7-4bbb-fe53-4ed4617bf026"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Up-\u0002propagation is an algorithm for inverting ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>We have constructed an inexpensive video based...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Non-negative matrix factorization (NMF) has pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Spike-triggered averaging techniques are effec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We consider continuous state, continuous actio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0    Up-\u0002propagation is an algorithm for inverting ...\n",
              "1    We have constructed an inexpensive video based...\n",
              "2    Non-negative matrix factorization (NMF) has pr...\n",
              "3    Spike-triggered averaging techniques are effec...\n",
              "4    We consider continuous state, continuous actio...\n",
              "Name: Abstract, dtype: object"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = df['Abstract']\n",
        "text.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlpFNJRN6rEY",
        "outputId": "68651d02-134d-4afc-d065-88e53656d1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words before text preprocessing: 732614\n",
            "Total number of unique words before text preprocessing:  41882\n",
            "['\"', '\"ALBO\"', '\"Air', '\"Air\\'\\'', '\"Answerer', '\"DIRECT\"', '\"DUOL\"', '\"Expansion-Constrained', '\"Generalized', '\"Ghost', '\"Graph', '\"GuessWhat?!\".', '\"Hedge\"', '\"Hey', '\"I', '\"Ising\\'\\'', '\"MNIST', '\"Object', '\"PixelGAN', '\"Self-Expressiveness', '\"Short-Dot\"', '\"TILT\\'\\'', '\"Ugliness-in-Averageness\"', '\"What', '\"additional', '\"anti-Bayesian\"', '\"autotags\")', '\"averagers,\"', '\"batch\"', '\"best', '\"body\\'\\'', '\"body\\'\\',', '\"building', '\"bus', '\"calibration', '\"catalyst\"', '\"chill\"', '\"comparison', '\"condition', '\"context\",', '\"convergence', '\"cooperative', '\"correctness\"', '\"date\"', '\"deep', '\"degrees', '\"deltas\",', '\"denoise\"', '\"describing\"', '\"determinantal', '\"disagreement', '\"disappearance\"', '\"discriminating\"', '\"discriminative', '\"early', '\"edge', '\"efficient\"', '\"em', '\"equalized', '\"exemplars\"', '\"expected', '\"extended', '\"external\"', '\"fair.\"', '\"few-shot\\'\\'', '\"filter', '\"follow-the-perturbed-leader\"', '\"found', '\"functional', '\"generalization', '\"generalized', '\"good\"', '\"graph-valued', '\"grown\"', '\"hard\"', '\"hiding\"', '\"hover', '\"human-like\"', '\"identity', '\"inherent\"', '\"intelligently\",', '\"interaction', '\"internal', '\"internal\"', '\"interrogators\",', '\"isotropic\"', '\"jogging\"', '\"kappa\"', '\"knows', '\"late', '\"latent\\'\\'', '\"latent\\'\\')', '\"learning', '\"local', '\"lucky\"', '\"manifold\"', '\"matching\"', '\"missingness\"', '\"multi-scale\"', '\"naive\"']\n",
            "['\"no\"', '\"none', '\"nonlinearity\"', '\"off-policyness\";', '\"optimal\"', '\"orientation\"', '\"overlapped\\'\\'', '\"partial', '\"patchily\",', '\"persistent', '\"pessimism', '\"pixel\"', '\"plain\"', '\"practice\"', '\"predictive', '\"privacy', '\"random', '\"reduced', '\"regression', '\"rendezvous\"', '\"s\"-sparse', '\"scaled', '\"self-expressiveness\"', '\"self-model\"', '\"shape\"of', '\"shared', '\"signature\"', '\"simple', '\"skeleton\"', '\"small', '\"small\"', '\"smoothed', '\"spectral', '\"standard\"', '\"status', '\"stragglers\"', '\"structured\",', '\"student\"', '\"successor', '\"surprise\",', '\"suspense\",', '\"targeted\"', '\"throw', '\"training', '\"travel\"', '\"trending', '\"two', '\"two-phase\\'\\'', '\"two-player', '\"typical', '\"ultra-slow\"', '\"universal\\'\\'', '\"visual', '\"volume', '\"walk', '\"what', '\"where', '\"world\"', '\"world-model\"', '#101.', '#86', '#P', '#P-hard', '#P-hard,', '#P-hard.', '#example', '$', '$(+\\\\!3,', '$(0,\\\\pi/2]$.', '$(1', '$(1+(1+\\\\epsilon)\\\\gamma)$-approximate', '$(1+O(\\\\psi))$-approximation', '$(1+\\\\alpha)\\\\,L^*_\\\\gamma', '$(1+\\\\eps)$', '$(1+\\\\eps)$-approximation', '$(1+\\\\epsilon)$', '$(1+\\\\epsilon)$-approximation', '$(1+\\\\epsilon)$-factor', '$(1+\\\\epsilon)z$,', '$(1+\\\\varepsilon)$', '$(1-1/\\\\sqrt{\\\\kappa})$', '$(1-1/e)$', '$(1-1/e)-\\\\epsilon$', '$(1-1/e)^2-\\\\delta$', '$(1-\\\\eps)f(S)', '$(1-\\\\epsilon)$-approximation', '$(1-\\\\varepsilon)^{\\\\ell}(1-1/e)$', '$(1/\\\\sqrt{n})$', '$(1/n)$', '$(1\\\\pm\\\\eps)$-coreset.', '$(2+\\\\epsilon)$-approximation', '$(2,', '$(2,1)$-norm', '$(3+o(1))/n^{1/3}$', '$(3,', '$(Christopher,', '$(L', '$(O(\\\\log^{-1}', '$(X_1,', '$(\\\\alpha,']\n",
            "['$(\\\\beta,B)$-Bernstein,', '$(\\\\delta,\\\\rho)$-mode', '$(\\\\delta,\\\\rho)$-modes', '$(\\\\ell,', '$(\\\\ell,\\\\mathcal{F},', '$(\\\\epsilon,', '$(\\\\epsilon,\\\\gamma)$-SOSP.', '$(\\\\epsilon,\\\\gamma)$-second', '$(\\\\frac{\\\\text{OPT}}{2}-\\\\epsilon)$.', '$(\\\\infty,', '$(\\\\varepsilon,', '$(d+1)$-partite', '$(d+1)n$', '$(has\\\\_husband,', '$(i)$', '$(i,j)$', '$(ii)$', '$(iii)$', '$(iv)$', '$(k+1)$-th', '$(k,', '$(m\\\\gg', '$(s,a)$,', '$(x_k)_{k=0}^K$,', '$(x_k)_{k=1}^K$', '$(y_k', '$+$', '$+1$', '$+1/\\\\sqrt{t}$', '$+\\\\!3$', '$-$in', '$-1$', '$-1/\\\\sqrt{t}$', '$0', '$0$', '$0$,', '$0.408', '$1', '$1$', '$1$)', '$1$,', '$1$-bit', '$1$-dimensional', '$1$-norm', '$1$.', '$1)$', '$1+\\\\Omega(\\\\frac{\\\\ln^2', '$1+\\\\epsilon$,', '$1+o(1)$.', '$1,', '$1,\\\\!000\\\\times$', '$1,\\\\infty$', '$1,\\\\infty$-regularized', '$1-1/e$', '$1-\\\\alpha$.', '$1-\\\\alpha_i$', '$1-\\\\delta$', '$1-\\\\delta$,', '$1-\\\\delta$.', '$1-c/e$', '$1-e^{-\\\\Omega(M)}$.', '$1.6\\\\%$', '$1/', '$1/(1-2\\\\eta)$,', '$1/2$', '$1/L$,', '$1/T$', '$1/\\\\alpha$', '$1/\\\\epsilon$', '$1/\\\\epsilon$.', '$1/\\\\sqrt{T}$', '$1/\\\\sqrt{n}$', '$1/\\\\sqrt{t}$', '$1/k$.', '$1/n^{\\\\gamma/c}$', '$1/p+1/p^{*}=1$,', '$1/t^2$', '$10$', '$10$.', '$10-20\\\\%$', '$100,\\\\!000$s', '$10\\\\times', '$10^4$.', '$10^8$.', '$10^{-12}$', '$10^{-3}$', '$10^{-6}$', '$10^{64}$', '$10^{70}$', '$11\\\\%$', '$121', '$13\\\\%$.', '$14', '$14\\\\%$', '$14\\\\epsilon', '$160', '$1\\\\le', '$1\\\\leq', '$2', '$2$.']\n"
          ]
        }
      ],
      "source": [
        "corpus_init = ' '.join(list(text))\n",
        "words_init = corpus_init.split()\n",
        "n_words_init = len(words_init)\n",
        "unique_words_init = sorted(list(set(words_init)))\n",
        "n_unique_words_init = len(unique_words_init)\n",
        "print(\"Total number of words before text preprocessing:\", n_words_init)\n",
        "print(\"Total number of unique words before text preprocessing: \", n_unique_words_init)\n",
        "print(unique_words_init[:100])\n",
        "print(unique_words_init[100:200])\n",
        "print(unique_words_init[200:300])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gWgjLvI2oz7"
      },
      "source": [
        "# Prétraitement de texte :\n",
        "Le but de cette étape est de préparer les données textuelles en les nettoyant et en les formatant de manière à ce qu'elles puissent être efficacement utilisées par le LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjg4O5AD7m_m"
      },
      "source": [
        "La fonction remove_non_ascii prend en entrée une liste de mots tokenisés et supprime les caractères non-ASCII de chaque mot. Elle utilise la normalisation Unicode NFKD (Normalization Form KC) pour décomposer les caractères accentués ou spéciaux en caractères de base, puis les encode en ASCII en ignorant les caractères non valides. Après cette étape, les mots sont décodés en UTF-8 pour obtenir une version \"nettoyée\" sans caractères non-ASCII"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kACCowUS43OY"
      },
      "outputs": [],
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word != ',' or word != '.':\n",
        "          new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "          new_words.append(new_word)\n",
        "    return new_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQRIMn6v76WK"
      },
      "source": [
        "La fonction clean_text est utilisée pour effectuer un prétraitement détaillé sur un texte. Voici les principales étapes :\n",
        "\n",
        "Suppression du bruit : Les URLs (liens), les références de citations (ex: \\cite{}), et les contenus entre crochets, parenthèses ou accolades sont supprimés ou remplacés par des mots-clés comme \"link\" ou \"cite\". Cela permet d'enlever les éléments qui n'ont pas d'importance pour l'analyse textuelle.\n",
        "\n",
        "Normalisation :\n",
        "\n",
        "Le texte est converti en minuscules pour éviter toute distinction entre les majuscules et les minuscules.\n",
        "Les mots avec des tirets ou des barres obliques sont séparés pour faciliter l'analyse.\n",
        "Les signes de ponctuation et autres caractères non alphanumériques sont retirés.\n",
        "Les numéros sont remplacés par le mot \"NUMBER\".\n",
        "Suppression des mots non-ASCII : Les caractères non-ASCII sont éliminés à l'aide de la fonction remove_non_ascii, qui nettoie les mots.\n",
        "\n",
        "Séparation des ponctuations : Les signes de ponctuation comme les points (.) et les virgules (,) sont séparés des mots pour être traités comme des entités distinctes.\n",
        "\n",
        "Conversion de l'anglais britannique en anglais américain : Certains mots d'orthographe britannique sont remplacés par leur équivalent américain, par exemple \"analysed\" devient \"analyzed\" et \"favourable\" devient \"favorable\".\n",
        "\n",
        "Cette fonction prépare donc le texte en supprimant les éléments non pertinents et en uniformisant les termes pour une meilleure compréhension et une analyse plus cohérente par les modèles de traitement du langage naturel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "DVp5L8b98ZqZ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  # Suppression du bruit\n",
        "  text = re.sub(r'\\bhttps?://\\w+.+[^ ]\\b', 'link', text)\n",
        "  text = re.sub(r'[a-zA-Z0-9]*\\.?github\\.?[a-zA-Z0-9]*','github',text)\n",
        "  text = re.sub(r'\\~\\\\cite\\{[^}]*\\}','cite',text)\n",
        "  text = re.sub(r'\\[[^]]*\\]', '', text)\n",
        "  text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "  text = re.sub(r'\\{[^)]*\\}', '', text)\n",
        "\n",
        "  # Normalisation\n",
        "  text = text.lower()  # Convertir en minuscules\n",
        "\n",
        "  text = re.sub(r'\\-',' ', text)  # Séparer les mots comme 'video-related'\n",
        "  text = re.sub(r'\\/',' ', text)  # Séparer les mots comme 'descriptors/tags'\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,]', '', text)  # Supprimer la ponctuation\n",
        "  text = re.sub(r'seq2seq', 'seqtoseq', text)\n",
        "\n",
        "  text = re.sub(r'[-+]?\\d*\\.?\\d+', 'NUMBER', text)  # Remplacer les nombres par \"NUMBER\"\n",
        "\n",
        "  text = re.sub(r'\\.{2,}', '', text)  # Supprimer '..','...'\n",
        "  text = re.sub(r'\\.', ' . ', text)  # Séparer '.' du texte\n",
        "  text = re.sub(r'\\,' , ' , ', text)  # Séparer ',' du texte\n",
        "\n",
        "  text = ' '.join(remove_non_ascii(text.split()))  # Supprimer les mots non-ASCII\n",
        "  text = re.sub(r'[\\w]*NUMBER[\\w]*', 'NUMBER', text)  # Remplacer tout mot contenant \"NUMBER\" par \"NUMBER\"\n",
        "\n",
        "  # Conversion de l'anglais britannique en anglais américain\n",
        "  text = re.sub(r'modelled', 'modeled', text)\n",
        "  text = re.sub(r'modelling', 'modeling', text)\n",
        "  text = re.sub(r'parallelisation', 'parallelization', text)\n",
        "  text = re.sub(r'parallelising', 'parallelizing', text)\n",
        "  text = re.sub(r'analysed', 'analyzed', text)\n",
        "  text = re.sub(r'generalised', 'generalized', text)\n",
        "  text = re.sub(r'maximisation', 'maximization', text)\n",
        "  text = re.sub(r'recogniser', 'recognizer', text)\n",
        "  text = re.sub(r'optimised', 'optimized', text)\n",
        "  text = re.sub(r'analyse', 'analyze', text)\n",
        "  text = re.sub(r'generalisation', 'generalization', text)\n",
        "  text = re.sub(r'generalised', 'generalized', text)\n",
        "  text = re.sub(r'factorisation', 'factorization', text)\n",
        "  text = re.sub(r'behaviour', 'behavior', text)\n",
        "  text = re.sub(r'interpretted', 'interpreted', text)\n",
        "  text = re.sub(r'neighbouring', 'neighboring', text)\n",
        "  text = re.sub(r'neighbour', 'neighbor', text)\n",
        "  text = re.sub(r'neighbours', 'neighbors', text)\n",
        "  text = re.sub(r'dependant', 'dependent', text)\n",
        "  text = re.sub(r'localisation', 'localization', text)\n",
        "  text = re.sub(r'amortised', 'amortized', text)\n",
        "  text = re.sub(r'amortisation', 'amortization', text)\n",
        "  text = re.sub(r'neutralising', 'neutralizing', text)\n",
        "  text = re.sub(r'prioritised', 'prioritized', text)\n",
        "  text = re.sub(r'characterised', 'characterized', text)\n",
        "  text = re.sub(r'characterise', 'characterize', text)\n",
        "  text = re.sub(r'centeralised', 'centeralized',text)\n",
        "  text = re.sub(r'initialisation', 'initialization', text)\n",
        "  text = re.sub(r'initialised', 'initialized', text)\n",
        "  text = re.sub(r'regularisation', 'regularization', text)\n",
        "  text = re.sub(r'regularised', 'regularized', text)\n",
        "  text = re.sub(r'optimisation', 'optimization', text)\n",
        "  text = re.sub(r'optimise', 'optimize', text)\n",
        "  text = re.sub(r'minimisation', 'minimization', text)\n",
        "  text = re.sub(r'generalises', 'generalizes', text)\n",
        "  text = re.sub(r'parameterised', 'parameterized', text)\n",
        "  text = re.sub(r'parameterises', 'parameterizes', text)\n",
        "  text = re.sub(r'reparameterisation', 'reparameterization', text)\n",
        "  text = re.sub(r'optimising', 'optimizing', text)\n",
        "  text = re.sub(r'favourable', 'favorable', text)\n",
        "  text = re.sub(r'hypothesised', 'hypothesized', text)\n",
        "  text = re.sub(r'summarise', 'summarize', text)\n",
        "  text = re.sub(r'standardised', 'standardized', text)\n",
        "  text = re.sub(r'randomisation', 'randomization', text)\n",
        "  text = re.sub(r'synchronisation', 'synchronization', text)\n",
        "  text = re.sub(r'travelling', 'traveling', text)\n",
        "\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "imAYtfCU43Oa"
      },
      "outputs": [],
      "source": [
        "text = text.apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Whp9pwI7VSX",
        "outputId": "a517b026-e0db-4687-94a9-d5c29aeba75b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of words: 783672\n",
            "Total number of unique words:  15274\n",
            "[',', '.', 'a', 'aa', 'aaai', 'aalen', 'aaronson', 'ab', 'abandons', 'abbe', 'abc', 'abdominal', 'aberrant', 'abf', 'abilities', 'ability', 'abilityto', 'ablation', 'able', 'ables', 'abnormal', 'abnormalities', 'abnormality', 'abound', 'abounds', 'about', 'above', 'abovethreshold', 'abp', 'abrupt', 'abscissa', 'absence', 'absent', 'absolute', 'absolutely', 'absorb', 'absorbed', 'absorbing', 'absorption', 'abstain', 'abstaining', 'abstention', 'abstract', 'abstracted', 'abstraction', 'abstractions', 'abstractionthe', 'abstractive', 'abstracts', 'abundance', 'abundancy', 'abundant', 'abuse', 'ac', 'academic', 'academics', 'accelerate', 'accelerated', 'accelerates', 'accelerating', 'acceleration', 'accelerators', 'accelerometers', 'accentuated', 'accept', 'acceptability', 'acceptable', 'acceptably', 'acceptance', 'accepted', 'accepts', 'access', 'accessed', 'accesses', 'accessibility', 'accessible', 'accessing', 'accident', 'accidental', 'accidents', 'acclaimed', 'accnet', 'accommodate', 'accommodated', 'accommodates', 'accomodate', 'accompanied', 'accompany', 'accompanying', 'accomplish', 'accomplished', 'accomplishes', 'accomplishing', 'accord', 'accordance', 'according', 'accordingly', 'account', 'accountable', 'accounted']\n",
            "['accounting', 'accounts', 'accp', 'accross', 'accumulate', 'accumulated', 'accumulating', 'accumulation', 'accumulations', 'accumulator', 'accumulators', 'accuracies', 'accuracy', 'accurate', 'accurately', 'acdm', 'ace', 'acetylcholine', 'achievability', 'achievable', 'achieve', 'achieved', 'achievements', 'achieves', 'achieving', 'achromatic', 'acid', 'acids', 'acknowledge', 'acknowledged', 'acknowledges', 'acnn', 'acnns', 'acoustic', 'acoustics', 'acquire', 'acquired', 'acquires', 'acquiring', 'acquisition', 'acquisitions', 'acronym', 'across', 'acrossa', 'acrossdatasets', 'acs', 'act', 'acted', 'acterization', 'acting', 'action', 'actionable', 'actions', 'activate', 'activated', 'activates', 'activating', 'activation', 'activations', 'active', 'actively', 'activities', 'activity', 'actor', 'actors', 'acts', 'actual', 'actually', 'actuated', 'actuation', 'acute', 'acutely', 'acyclic', 'acyclicity', 'ad', 'ada', 'adaboost', 'adadelta', 'adagan', 'adagrad', 'adam', 'adams', 'adamsmoulton', 'adap', 'adapt', 'adaptability', 'adaptation', 'adaptations', 'adapted', 'adapter', 'adapting', 'adaption', 'adaptive', 'adaptively', 'adaptiveness', 'adaptiverevision', 'adaptivity', 'adaptor', 'adapts', 'add']\n",
            "['added', 'adding', 'addition', 'additional', 'additionally', 'additions', 'additive', 'additivity', 'address', 'addressed', 'addresses', 'addressing', 'adds', 'ade', 'adept', 'adequacy', 'adequate', 'adequately', 'ader', 'adheres', 'adhering', 'adjacency', 'adjacent', 'adjusments', 'adjust', 'adjustable', 'adjusted', 'adjusting', 'adjustment', 'adjustments', 'adjusts', 'adm', 'administered', 'admira', 'admissibility', 'admissible', 'admission', 'admissions', 'admit', 'admits', 'admitting', 'admixture', 'admixtures', 'admm', 'adni', 'ado', 'adolescent', 'adopt', 'adopted', 'adopting', 'adoption', 'adopts', 'adp', 'adpp', 'adress', 'adresses', 'ads', 'adults', 'advance', 'advanced', 'advancement', 'advancements', 'advances', 'advancing', 'advantage', 'advantageous', 'advantages', 'advection', 'advent', 'adversarial', 'adversarially', 'adversaries', 'adversary', 'adversarys', 'adverse', 'adversely', 'advertise', 'advertisement', 'advertisements', 'advertiser', 'advertisers', 'advertising', 'advi', 'advice', 'advise', 'advised', 'advocate', 'advocated', 'adwords', 'aen', 'aer', 'aerial', 'aesthetic', 'aesthetically', 'aesthetics', 'afd', 'affairs', 'affect', 'affected', 'affecting']\n",
            "['affects', 'afferent', 'afferents', 'affiliation', 'affine', 'affinities', 'affinity', 'affirm', 'affirmation', 'affirmative', 'affirmatively', 'afford', 'affordable', 'afforded', 'affording', 'affords', 'afhmm', 'aforementioned', 'aforesaid', 'african', 'after', 'aftereffects', 'aftern', 'afterwards', 'ag', 'again', 'against', 'age', 'agency', 'agenda', 'agendas', 'agent', 'agents', 'agglomerate', 'agglomeration', 'agglomerations', 'agglomerative', 'aggregate', 'aggregated', 'aggregates', 'aggregating', 'aggregation', 'aggregations', 'aggregative', 'aggressive', 'aggressively', 'agiven', 'agm', 'agnos', 'agnostic', 'agnostically', 'agnostophobia', 'ago', 'agorithm', 'agree', 'agreement', 'agrees', 'ags', 'ahead', 'ahp', 'ahundred', 'ai', 'aic', 'aid', 'aide', 'aided', 'aiding', 'aids', 'aim', 'aimathbf', 'aimbrain', 'aimed', 'aiming', 'aims', 'air', 'airborne', 'aircraft', 'airflow', 'airline', 'ais', 'ait', 'ak', 'aka', 'akaike', 'akin', 'al', 'alarm', 'alarmingly', 'alarms', 'albeit', 'albo', 'album', 'alcohol', 'aldous', 'ale', 'aleatoric', 'alert', 'alexanders', 'alexnet', 'algebra']\n",
            "['algebraic', 'algebraically', 'algo', 'algoirthm', 'algorithm', 'algorithmcalled', 'algorithmcite', 'algorithmic', 'algorithmically', 'algorithms', 'algorithmsa', 'algorithmshave', 'algorithmthat', 'aliasing', 'alice', 'aligment', 'align', 'aligned', 'aligning', 'alignment', 'alignments', 'aligns', 'alike', 'alization', 'all', 'allay', 'allegedly', 'allen', 'alleviate', 'alleviated', 'alleviates', 'alleviating', 'alligned', 'allocate', 'allocated', 'allocates', 'allocating', 'allocation', 'allocations', 'allow', 'allowable', 'allowed', 'allowing', 'allows', 'allowsnatural', 'allowssubset', 'alloy', 'allreduce', 'almost', 'alone', 'along', 'alongside', 'aloocv', 'alp', 'alpaca', 'alpha', 'alphabet', 'alphabets', 'alphacsc', 'alphago', 'alphai', 'alphain', 'alphanumeric', 'alphawidetilde', 'already', 'als', 'also', 'alter', 'alteration', 'alterations', 'altered', 'altering', 'alternate', 'alternated', 'alternately', 'alternates', 'alternating', 'alternation', 'alternations', 'alternative', 'alternatively', 'alternatives', 'altest', 'although', 'altitude', 'altogether', 'always', 'alzheimer', 'alzheimers', 'am', 'amalgamates', 'amazing', 'amazon', 'ambient', 'ambiguities', 'ambiguity', 'ambiguous', 'ambitious', 'ambitiously', 'amenability']\n",
            "['amenable', 'amend', 'amendable', 'amended', 'american', 'amini', 'amino', 'ammar', 'amoebe', 'among', 'amongst', 'amortization', 'amortize', 'amortized', 'amortizes', 'amount', 'amounts', 'amp', 'ample', 'amplification', 'amplified', 'amplify', 'amplifying', 'amplitude', 'amplitudes', 'an', 'analog', 'analogical', 'analogies', 'analogous', 'analogously', 'analogs', 'analogue', 'analogues', 'analogy', 'analysing', 'analysis', 'analysissynthesis', 'analysisusing', 'analyst', 'analysts', 'analytic', 'analytical', 'analytically', 'analyticity', 'analytics', 'analyze', 'analyzed', 'analyzer', 'analyzers', 'analyzes', 'analyzing', 'anarchy', 'anatomic', 'anatomical', 'anatomically', 'anatomy', 'ance', 'ancestor', 'ancestral', 'anchor', 'anchored', 'anchoring', 'anchors', 'ancient', 'ancillary', 'and', 'andare', 'andeigenvalue', 'anderson', 'andersson', 'android', 'anecdotal', 'anechoic', 'anencoder', 'anesthesia', 'anesthetized', 'anew', 'angle', 'angles', 'angluins', 'angular', 'animal', 'animals', 'animated', 'animation', 'anisotropic', 'anisotropism', 'anlysis', 'anms', 'ann', 'anneal', 'annealed', 'annealing', 'annotate', 'annotated', 'annotating', 'annotation', 'annotations', 'annotator']\n",
            "['annotators', 'announced', 'anns', 'annual', 'anomalies', 'anomalous', 'anomaly', 'anonymity', 'anonymize', 'another', 'anothers', 'anova', 'answer', 'answerer', 'answerers', 'answering', 'answers', 'ant', 'antagonistic', 'ante', 'anteed', 'antemakes', 'antennal', 'anterior', 'anthropomorphic', 'anti', 'anticipate', 'anticipated', 'anticipation', 'anticipative', 'anticipatory', 'antisense', 'antithetic', 'ants', 'any', 'anyclassifier', 'anymore', 'anything', 'anytime', 'anyway', 'aois', 'ap', 'apache', 'apart', 'apascal', 'apcg', 'aperture', 'apg', 'api', 'apical', 'aplanted', 'apm', 'aposteriori', 'app', 'apparatus', 'apparent', 'apparently', 'appeal', 'appealing', 'appear', 'appearance', 'appearances', 'appeared', 'appearin', 'appearing', 'appears', 'append', 'appled', 'appli', 'appliance', 'appliances', 'applica', 'applicability', 'applicable', 'applicant', 'applicants', 'application', 'applications', 'applicationshave', 'applicationsin', 'applicationsneural', 'applied', 'applies', 'apply', 'applying', 'appreciate', 'appreciated', 'apprentices', 'apprenticeship', 'approach', 'approached', 'approaches', 'approachesbecause', 'approachessatisfying', 'approachhere', 'approaching', 'approachis', 'approachs', 'approch', 'approporiate']\n"
          ]
        }
      ],
      "source": [
        "text_list = list(text)\n",
        "corpus = ' '.join(text_list)\n",
        "words = corpus.split()\n",
        "n_words = len(words)\n",
        "unique_words = sorted(list(set(words)))\n",
        "n_unique_words = len(unique_words)\n",
        "print(\"Total number of words:\", n_words)\n",
        "print(\"Total number of unique words: \", n_unique_words)\n",
        "print(unique_words[:100])\n",
        "print(unique_words[100:200])\n",
        "print(unique_words[200:300])\n",
        "print(unique_words[300:400])\n",
        "print(unique_words[400:500])\n",
        "print(unique_words[500:600])\n",
        "print(unique_words[600:700])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_me3HV2h43Ob"
      },
      "source": [
        "# Sauvegarde du texte nettoyé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-57bCiZk43Oc"
      },
      "outputs": [],
      "source": [
        "text_str = '\\n'.join(text_list)\n",
        "with open(\"nips_clean.txt\", 'w') as file:\n",
        "  file.write(text_str)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
