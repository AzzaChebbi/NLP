{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B46pOmSq6IgD"
      },
      "outputs": [],
      "source": [
        "#!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsHzd9165530"
      },
      "source": [
        "# Ici on charge le modèle qu'on a entrainé"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klfmQEUz91vU",
        "outputId": "d50172a3-f413-4f6c-e7c6-f0a3ee07f9a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fichiers dans le dossier : ['checkpoint', 'counter', 'encoder.json', 'events.out.tfevents.1739625350.049ba51cbd2b', 'events.out.tfevents.1739627838.049ba51cbd2b', 'events.out.tfevents.1739632550.049ba51cbd2b', 'hparams.json', 'model-5000.data-00000-of-00001', 'model-5000.index', 'model-5000.meta', 'vocab.bpe']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "chemin_du_model = \"./run1\"\n",
        "print(\"Fichiers dans le dossier :\", os.listdir(chemin_du_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pk50ji591vV",
        "outputId": "e61e14b3-a12f-4ee1-c977-1a531f9533b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chemin absolu du modèle : c:\\Users\\GAMING\\Downloads\\text-generation-master\\text-generation-master\\run1\n",
            "Contenu du dossier : ['checkpoint', 'counter', 'encoder.json', 'events.out.tfevents.1739625350.049ba51cbd2b', 'events.out.tfevents.1739627838.049ba51cbd2b', 'events.out.tfevents.1739632550.049ba51cbd2b', 'hparams.json', 'model-5000.data-00000-of-00001', 'model-5000.index', 'model-5000.meta', 'vocab.bpe']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "chemin_du_model = \"./run1\"  # essai avec le chemin absolu\n",
        "print(\"Chemin absolu du modèle :\", os.path.abspath(chemin_du_model))\n",
        "print(\"Contenu du dossier :\", os.listdir(chemin_du_model))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwWytSw291vW",
        "outputId": "387ba68b-df78-4942-fc3c-51d3786642ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dossier checkpoint défini : c:\\Users\\GAMING\\Downloads\\text-generation-master\\text-generation-master\\run1\n",
            "WARNING:tensorflow:From c:\\Users\\GAMING\\Downloads\\text-generation-master\\text-generation-master\\envnlp\\Lib\\site-packages\\gpt_2_simple\\gpt_2.py:401: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint ./run1\\model-5000\n",
            "INFO:tensorflow:Restoring parameters from ./run1\\model-5000\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.environ['CHECKPOINT_DIR'] = os.path.abspath(\"./run1\")\n",
        "print(\"Dossier checkpoint défini :\", os.environ['CHECKPOINT_DIR'])\n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import tensorflow as tf\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name=\"run1\", checkpoint_dir=\"./\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSosz_1n7JkY"
      },
      "source": [
        "# Tester la génération de texte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCSHqPjZ7bXw"
      },
      "outputs": [],
      "source": [
        "seed = \"in deep neural nets, lower level embedding layers account for a large portion of the total number of parameters.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XNXqpoF91vZ",
        "outputId": "e7698866-f1be-4ea5-8198-a94db24fa67d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. however , such layers are difficult to train and often hard to interpret . we present a novel method to train which tensor decomposes into several smaller subspaces , where each subspace is represented by a few shot representation of the next subspace . we show that for any high dimensional subspace , one can efficiently learn where each subspace comes from without having to know the representation of the space before training starts . we show that this representation learning algorithm can be used to train deep embeddings which\n",
            "====================\n",
            "in deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. we show how deep neural nets can be trained such that the resulting residual matrices are efficiently diagonalizable . we show that mtlns can be interpreted as two separate problemsing shared residual matrices under differential privacy and sharing the learned residual matrices under non private conditions . while the latter is challenging to solve , we explain the differential privacy resulting in the low noise setting achieving o regret . in the noisy setting , we show that the joint losses of each layers input and output are described by a set of\n",
            "====================\n",
            "in deep neural nets, lower level embedding layers account for a large portion of the total number of parameters. we show that , compared to the parametric neural networks , their parameter reductions translate into reductions in the number of parameters processed per second , the gain of the proposed lower layer . we discuss the effects of the proposed embeddings on the training of convnets , particularly their increased interpretability .\n",
            "the growing prospect of deep architectures learning speculators is already having preliminary evaluation , with several corpora with potentially large capacity implications pulled from multiplexes on storage . we anticipate that these storage embargoes will prove\n",
            "====================\n"
          ]
        }
      ],
      "source": [
        "gpt2.generate(sess,\n",
        "              length=100,  # default 1023, the maximum\n",
        "              temperature=0.7,\n",
        "              prefix=seed,\n",
        "              nsamples=3,\n",
        "              batch_size=3,\n",
        "              checkpoint_dir=\"./\"\n",
        "              )\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "envnlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}