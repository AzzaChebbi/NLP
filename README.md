# NLP
## Table des mati√®res
- [Collecte de Donn√©es](#collecte_de_donn√©es)
- [Pr√©traitement des Donn√©es](#pr√©traitement_des_donn√©es)
- [Mod√®les de G√©n√©ration de Texte](#mod√®les_de_g√©n√©ration_de_texte)
- [G√©n√©ration de Texte](#g√©n√©ration_de_texte)
- [Clustering Th√©matique](#clustering_th√©matique)
- [Interface Utilisateur](#interface_utilisateur)

## Collecte de Donn√©es
Le script web_scraping.py extrait des textes scientifiques et les sauvegarde au format CSV.
```bash
python web_scraping.py
```
Output: nips.csv
## Pr√©traitement des Donn√©es
Le notebook text_preprocessing.ipynb nettoie et pr√©pare les donn√©es textuelles pour l'entra√Ænement.

Op√©rations principales:

 üìÇ Importation et Exploration des Donn√©es
  
   - Chargement du dataset nips.csv depuis Google Drive.
   - Suppression des abstracts manquants.
   - Exploration des donn√©es : nombre d'entr√©es, types de colonnes et aper√ßu du texte.
  
 üîç Pr√©traitement du Texte

   - Suppression des caract√®res non-ASCII et du bruit (URLs, citations, ponctuation).
   - Normalisation du texte : conversion en minuscules, remplacement des nombres par "NUMBER".
   - Standardisation des termes en anglais am√©ricain.
   - Tokenisation et analyse du vocabulaire (nombre total et unique de mots).

Output: nips_clean.txt


## Mod√®les de G√©n√©ration de Texte
#### 3.1 Mod√®le LSTM :   Ce notebook impl√©mente un mod√®le de g√©n√©ration de texte en utilisant un r√©seau de neurones r√©current (RNN) bas√© sur une architecture LSTM bidirectionnelle.

 Pr√©paration des donn√©es
 - Chargement du corpus : Le texte utilis√© provient d'un fichier contenant des articles de NIPS.
 - Nettoyage des donn√©es : Suppression des mots rares (fr√©quence < 3) pour √©viter le bruit dans l'entra√Ænement.
 - Tokenisation : Conversion des mots en s√©quences d'entiers √† l'aide de Tokenizer de Keras.
 - Cr√©ation des s√©quences : G√©n√©ration de s√©quences de longueur fixe (200 mots), avec la derni√®re position utilis√©e comme cible pour la pr√©diction.

 Entra√Ænement du mod√®le
  - Une couche d'embedding (256 dimensions) pour capturer les relations entre les mots.
  - Une couche LSTM bidirectionnelle (128 unit√©s) pour exploiter les d√©pendances dans les deux directions du texte.
  - Une couche de dropout (0.2) pour √©viter l'overfitting.
  - Une couche dense avec activation softmax pour la pr√©diction du mot suivant.
  - Compilation avec la fonction de perte categorical_crossentropy et l'optimiseur Adam.
  - Callback EarlyStopping pour stopper l'entra√Ænement si la perte ne diminue plus apr√®s 5 epochs.
  - G√©n√©ration des batchs : Une fonction de g√©n√©rateur est utilis√©e pour alimenter le mod√®le avec les donn√©es en m√©moire de mani√®re efficace.
  - Entra√Ænement effectu√© sur 300 epochs avec un batch size de 32.
 
 Sauvegarde du mod√®le
  - Mod√®le enregistr√© sous model.h5 pour une future r√©utilisation.
  - Tokenizer sauvegard√© (tokenizer.pkl) pour conserver l'indexation des mots.
  - S√©quences de mots enregistr√©es (sequences_words.txt) pour une √©ventuelle reprise du pr√©traitement.

#### 3.2 Fine-tuning de GPT-2 (Mod√®le retenu) : Le notebook `gpt2_train.ipynb` adapte le mod√®le pr√©-entra√Æn√© GPT-2 √† notre corpus scientifique.

   - Adaptation du mod√®le au domaine scientifique  
   - Perte plus faible compar√©e √† celle du mod√®le entra√Æn√© from scratch  
   - Meilleure capacit√© de g√©n√©ralisation  

Ce notebook impl√©mente un fine-tuning du mod√®le GPT-2 (124M) sur un corpus sp√©cifique afin de g√©n√©rer du texte adapt√© aux donn√©es fournies.

  - Installation et Pr√©paration de l‚ÄôEnvironnement
  - Installation de la biblioth√®que gpt-2-simple pour faciliter l‚Äôutilisation de GPT-2.
  - Importation des biblioth√®ques n√©cessaires (gpt_2_simple, tensorflow).
  - T√©l√©chargement du mod√®le pr√©-entra√Æn√© GPT-2 (version 124M).
  - Montage de Google Drive pour stocker et r√©cup√©rer les fichiers d‚Äôentra√Ænement.
  - Copie du fichier de texte nips_clean.txt depuis Google Drive vers l‚Äôenvironnement Colab.

Entra√Ænement du mod√®le sur un corpus sp√©cifique
  - D√©marrage d‚Äôune session TensorFlow pour g√©rer l‚Äôentra√Ænement du mod√®le.
  - Fine-tuning initial :
    - Chargement du corpus.
    - Entra√Ænement du mod√®le avec steps=1000.
    - Sauvegarde automatique du mod√®le toutes les 500 it√©rations.
    - G√©n√©ration d‚Äô√©chantillons de texte toutes les 200 it√©rations pour √©valuer la progression.
  - Continuation du fine-tuning :
    - R√©initialisation de la session TensorFlow et chargement du dernier checkpoint.
    - Extension de l‚Äôentra√Ænement √† 2000 steps suppl√©mentaires.
    - Nouvelle reprise avec 2000 steps suppl√©mentaires pour am√©liorer la qualit√© de la g√©n√©ration.

Sauvegarde et Chargement du Mod√®le Entra√Æn√©
 - Sauvegarde des checkpoints sur Google Drive pour √©viter toute perte de progr√®s.
 - R√©cup√©ration des checkpoints en extrayant les fichiers du mod√®le depuis Google Drive vers l‚Äôenvironnement Colab.
 - V√©rification du contenu du dossier run1 contenant le mod√®le entra√Æn√©.G√©n√©ration de texte avec le mod√®le fine-tun√©

Chargement du mod√®le entra√Æn√© (run1).
 - G√©n√©ration de texte en utilisant le mod√®le ajust√© sur le corpus sp√©cifique.
## G√©n√©ration de Texte
Le notebook gpt2_generate.ipynb utilise le mod√®le GPT-2 fine-tun√© pour g√©n√©rer de nouveaux textes scientifiques.

Apr√®s avoir √©valu√© les 2 mod√®les, nous avons opt√© pour le mod√®le GPT-2 fine-tun√© en raison de sa performance optimale, notamment gr√¢ce √† la minimisation de la fonction de perte (loss)
## Clustering Th√©matique
Le notebook de clustering applique l'algorithme K-means pour regrouper les textes scientifiques par th√©matique ou domaine.

√âtapes:

  - Vectorisation des textes (TF-IDF)
  - Classification des documents
  - Visualisation des clusters

## Interface Utilisateur
L'application Streamlit (app.py) int√®gre toutes les fonctionnalit√©s dans une interface conviviale.

Pour lancer l'application:

```bash
streamlit run app.py
```

Voil√† des captures de notre application : 

![Result_NLP1](https://github.com/user-attachments/assets/0dfcd9a7-56b7-4f73-ad44-b737988b00ab)
