# NLP
## Table des mati√®res
- [Collecte de Donn√©es](#collecte_de_donn√©es)
- [Pr√©traitement des Donn√©es](#pr√©traitement_des_donn√©es)
- [Mod√®les de G√©n√©ration de Texte](#mod√®les_de_g√©n√©ration_de_texte)
- [G√©n√©ration de Texte](#g√©n√©ration_de_texte)
- [Clustering Th√©matique](#clustering_th√©matique)
- [Interface Utilisateur](#interface_utilisateur)

## Collecte de Donn√©es
Le script web_scraping.py extrait des textes scientifiques et les sauvegarde au format CSV.
```bash
python web_scraping.py
```
Output: nips.csv
## Pr√©traitement des Donn√©es
Le notebook text_preprocessing.ipynb nettoie et pr√©pare les donn√©es textuelles pour l'entra√Ænement.

Op√©rations principales:

 üìÇ Importation et Exploration des Donn√©es
  
   - Chargement du dataset nips.csv depuis Google Drive.
   - Suppression des abstracts manquants.
   - Exploration des donn√©es : nombre d'entr√©es, types de colonnes et aper√ßu du texte.
  
 üîç Pr√©traitement du Texte

   - Suppression des caract√®res non-ASCII et du bruit (URLs, citations, ponctuation).
   - Normalisation du texte : conversion en minuscules, remplacement des nombres par "NUMBER".
   - Standardisation des termes en anglais am√©ricain.
   - Tokenisation et analyse du vocabulaire (nombre total et unique de mots).

Output: nips_clean.txt


## Mod√®les de G√©n√©ration de Texte
#### 3.1 Mod√®le LSTM :   Ce notebook impl√©mente un mod√®le de g√©n√©ration de texte en utilisant un r√©seau de neurones r√©current (RNN) bas√© sur une architecture LSTM bidirectionnelle.

 Pr√©paration des donn√©es
 - Chargement du corpus : Le texte utilis√© provient d'un fichier contenant des articles de NIPS.
 - Nettoyage des donn√©es : Suppression des mots rares (fr√©quence < 3) pour √©viter le bruit dans l'entra√Ænement.
 - Tokenisation : Conversion des mots en s√©quences d'entiers √† l'aide de Tokenizer de Keras.
 - Cr√©ation des s√©quences : G√©n√©ration de s√©quences de longueur fixe (200 mots), avec la derni√®re position utilis√©e comme cible pour la pr√©diction.

 Entra√Ænement du mod√®le
  - Une couche d'embedding (256 dimensions) pour capturer les relations entre les mots.
  - Une couche LSTM bidirectionnelle (128 unit√©s) pour exploiter les d√©pendances dans les deux directions du texte.
  - Une couche de dropout (0.2) pour √©viter l'overfitting.
  - Une couche dense avec activation softmax pour la pr√©diction du mot suivant.
  - Compilation avec la fonction de perte categorical_crossentropy et l'optimiseur Adam.
  - Callback EarlyStopping pour stopper l'entra√Ænement si la perte ne diminue plus apr√®s 5 epochs.
  - G√©n√©ration des batchs : Une fonction de g√©n√©rateur est utilis√©e pour alimenter le mod√®le avec les donn√©es en m√©moire de mani√®re efficace.
  - Entra√Ænement effectu√© sur 300 epochs avec un batch size de 32.
 
 Sauvegarde du mod√®le
  -Mod√®le enregistr√© sous model.h5 pour une future r√©utilisation.
  -Tokenizer sauvegard√© (tokenizer.pkl) pour conserver l'indexation des mots.
  -S√©quences de mots enregistr√©es (sequences_words.txt) pour une √©ventuelle reprise du pr√©traitement.

#### 3.2 Fine-tuning de GPT-2 (Mod√®le retenu)

  Le notebook `gpt2_train.ipynb` adapte le mod√®le pr√©-entra√Æn√© GPT-2 √† notre corpus scientifique.

   - Adaptation du mod√®le au domaine scientifique  
   - Perte plus faible compar√©e √† celle du mod√®le entra√Æn√© from scratch  
   - Meilleure capacit√© de g√©n√©ralisation  

## G√©n√©ration de Texte
Le notebook gpt2_generate.ipynb utilise le mod√®le GPT-2 fine-tun√© pour g√©n√©rer de nouveaux textes scientifiques.

## Clustering Th√©matique
Le notebook de clustering applique l'algorithme K-means pour regrouper les textes scientifiques par th√©matique ou domaine.

√âtapes:

  - Vectorisation des textes (TF-IDF)
  - Classification des documents
  - Visualisation des clusters

## Interface Utilisateur
L'application Streamlit (app.py) int√®gre toutes les fonctionnalit√©s dans une interface conviviale.

Pour lancer l'application:

```bash
streamlit run app.py
```
